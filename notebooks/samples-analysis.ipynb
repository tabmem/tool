{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Plots of samples on the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# append the parent directory to the path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import jellyfish\n",
    "\n",
    "import yaml\n",
    "\n",
    "import experiment_utils\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "import csv\n",
    "\n",
    "import analysis \n",
    "import utils\n",
    "\n",
    "# re-load upon module change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the samples on all the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['IRIS', 'uci-wine', 'sklearn-diabetes', 'titanic-train',  'openml-diabetes', 'adult', 'california-housing']\n",
    "\n",
    "original_data = {}\n",
    "gpt4_samples = {}\n",
    "gpt35_samples = {}\n",
    "for dataset in datasets:\n",
    "    df_data =  utils.load_csv_df(f'../csv/{dataset}.csv', dtype=str)\n",
    "    df_gpt35 = pd.read_csv(f'../results/gpt-3.5-turbo/samples/{dataset}-temperature-0.7.csv', dtype=str)\n",
    "    df_gpt4 = pd.read_csv(f'../results/gpt-4-32k-0314/samples/{dataset}-temperature-0.7.csv', dtype=str)\n",
    "    original_data[dataset] = df_data\n",
    "    gpt4_samples[dataset] = df_gpt4\n",
    "    gpt35_samples[dataset] = df_gpt35\n",
    "\n",
    "# fico\n",
    "datasets.append('fico')\n",
    "df_data = pd.read_csv(f'../../private-do-not-distribute/fico.csv', dtype=str)\n",
    "df_gpt35 = pd.read_csv(f'../../private-do-not-distribute/results/fico-samples-gpt-3.5-temperature-0.7.csv', dtype=str)\n",
    "df_gpt4 = pd.read_csv(f'../../private-do-not-distribute/results/fico-samples-gpt-4-temperature-0.7.csv', dtype=str)\n",
    "\n",
    "original_data['fico'] = df_data\n",
    "gpt4_samples['fico'] = df_gpt4\n",
    "gpt35_samples['fico'] = df_gpt35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatted feature names\n",
    "formatted_feature_names = {'IRIS': ['S-Length', 'S-Width', 'P-Length', 'P-Width', 'Species'],\n",
    "'uci-wine': ['T', 'Alc', 'MAc', 'Ash', 'Alca', 'Mag', 'Phen', 'Flav', 'NFP', 'Pro.', 'Inten', 'Hue', 'od', 'Prol'],\n",
    "'sklearn-diabetes': ['Age', 'Sex', 'bmi', 'bp', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'Y'],\n",
    "'titanic-train': ['Id', 'Surv', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embkd'],\n",
    "'openml-diabetes': ['Preg', 'Gluc', 'BP', 'STkns', 'Insul', 'BMI', 'DiaPF', 'Age', 'Out'],\n",
    "'adult': ['Age', 'Work', 'fnl', 'Edu', 'EduNum', 'Mar', 'Occ', 'Rel', 'Race', 'Gen', 'CGain', 'CLoss', 'Hours', 'Coun', 'Inc'],\n",
    "'california-housing': ['Long', 'Lat', 'MAge', 'NR', 'NBR', 'Pop', 'Hou', 'Inc', 'Val', 'Oce'],\n",
    "'fico': ['RP', 'ERE', 'MSO', 'MSM', 'AMIF', 'NST', 'NT60', 'NT90', 'PTND', 'MSMR', 'MDP12']}\n",
    "\n",
    "for d in datasets:\n",
    "    fnn = formatted_feature_names[d]\n",
    "    colnames = list(original_data[d].columns)\n",
    "    colnames[:len(fnn)] = fnn\n",
    "\n",
    "    original_data[d].columns = colnames\n",
    "    gpt4_samples[d].columns = colnames\n",
    "    gpt35_samples[d].columns = colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    df_data = original_data[dataset].copy(deep=True)\n",
    "    df_gpt35 = gpt35_samples[dataset].copy(deep=True)\n",
    "    df_gpt4 = gpt4_samples[dataset].copy(deep=True)\n",
    "    # print head for all datasets, without line break\n",
    "    print(dataset)\n",
    "    print(df_data.head(5).to_string(index=False, header=False, line_width=1000))\n",
    "    print(df_gpt35.head(5).to_string(index=False, header=False, line_width=1000))\n",
    "    print(df_gpt4.head(5).to_string(index=False, header=False, line_width=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation table for all datasets (at most 10 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "#for dataset in ['titanic-train', 'california-housing']:\n",
    "    print(dataset)\n",
    "    df_data = original_data[dataset].copy(deep=True)\n",
    "    df_gpt35 = gpt35_samples[dataset].copy(deep=True)\n",
    "    df_gpt4 = gpt4_samples[dataset].copy(deep=True)\n",
    "    \n",
    "    for df in [df_data, df_gpt35, df_gpt4]:\n",
    "        float_variables = []\n",
    "        for var in df.columns:\n",
    "            try:\n",
    "                df[var] = df[var].astype(float)\n",
    "                float_variables.append(var)\n",
    "            except:\n",
    "                # drop column\n",
    "                df.drop(columns=[var], inplace=True)\n",
    "        # drop all other columns\n",
    "        df.drop(columns=[var for var in df.columns if var not in float_variables], inplace=True)\n",
    "\n",
    "\n",
    "    # keep only the first features in each dataframe\n",
    "    df_data = df_data.iloc[:, :8]\n",
    "    df_gpt35 = df_gpt35.iloc[:, :8]\n",
    "    df_gpt4 = df_gpt4.iloc[:, :8]\n",
    "\n",
    "    # increase font size\n",
    "    sns.set(font_scale=1.4)\n",
    "\n",
    "    ############ JOINT PLOT ############\n",
    "    # a figure with 3 heatmaps, one for each dataset.\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(25, 5), layout='compressed')\n",
    "    v_min = -1\n",
    "    v_max = 1\n",
    "\n",
    "    sns.heatmap(df_data.corr(), annot=df_data.corr(), fmt='.2f', vmin=v_min, vmax=v_max, ax=axs[0], cmap=sns.diverging_palette(20, 220, n=200), square=True, annot_kws={\"size\": 14})\n",
    "    sns.heatmap(df_gpt35.corr(), annot=True, fmt='.2f', vmin=v_min, vmax=v_max, ax=axs[1], cmap=sns.diverging_palette(20, 220, n=200), square=True, annot_kws={\"size\": 14})\n",
    "    sns.heatmap(df_gpt4.corr(), annot=True, fmt='.2f', vmin=v_min, vmax=v_max, ax=axs[2], cmap=sns.diverging_palette(20, 220, n=200), square=True, annot_kws={\"size\": 14})\n",
    "\n",
    "\n",
    "    axs[1].set_yticks([])\n",
    "    axs[2].set_yticks([])\n",
    "    # remove the colorbar from the first two heatmaps\n",
    "    axs[0].collections[0].colorbar.remove()\n",
    "    axs[1].collections[0].colorbar.remove()\n",
    "    axs[2].collections[0].colorbar.remove()\n",
    "    # add colorbar to the right of the last heatmap\n",
    "    #fig.colorbar(axs[2].collections[0], ax=axs[2], location='right')\n",
    "    \n",
    "    # have a bit of a gap between the plots\n",
    "    #plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "    #axs[0].set_title(dataset)\n",
    "    #axs[1].set_title('GPT-3.5')\n",
    "    #axs[2].set_title('GPT-4')\n",
    "\n",
    "    # save\n",
    "    plt.savefig(f'figures/{dataset}-heatmap.png', dpi=600, bbox_inches='tight')\n",
    "\n",
    "    axs[0].set_title('Dataset')\n",
    "    axs[1].set_title('GPT-3.5')\n",
    "    axs[2].set_title('GPT-4')  \n",
    "\n",
    "    plt.savefig(f'figures/{dataset}-heatmap.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    ############ INDIVIDUAL PLOTS ############\n",
    "    for corr, name in [(df_data.corr(), 'data'), (df_gpt35.corr(), 'gpt35'), (df_gpt4.corr(), 'gpt4')]:\n",
    "        # plot the correlation matrix\n",
    "        fig, ax = plt.subplots(figsize=(7, 7))\n",
    "        sns.heatmap(corr, annot=True, fmt='.2f', vmin=v_min, vmax=v_max, ax=ax, cmap=sns.diverging_palette(20, 220, n=200), square=True, annot_kws={\"size\": 14})\n",
    "        ax.collections[0].colorbar.remove()\n",
    "        plt.savefig(f'figures/{dataset}-heatmap-{name}.png', dpi=600, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fraction of samples from data vs. fraction of values form data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    samples35_df = gpt35_samples[dataset]\n",
    "    samples4_df = gpt4_samples[dataset]\n",
    "    data_df = original_data[dataset]\n",
    "        \n",
    "    print(dataset)\n",
    "    # for all rows in sample_df, check if it is in data_df (as an entire row)\n",
    "    num_in_df = 0\n",
    "    for i in range(samples35_df.shape[0]):\n",
    "        row = samples35_df.iloc[i]\n",
    "        if analysis.is_in_df(data_df, row):\n",
    "            num_in_df += 1\n",
    "    print('GPT-3.5-turbo', num_in_df / samples35_df.shape[0])\n",
    "\n",
    "    # for all rows in sample_df, check if it is in data_df (as an entire row)\n",
    "    num_in_df = 0\n",
    "    for i in range(samples4_df.shape[0]):\n",
    "        row = samples4_df.iloc[i]\n",
    "        if analysis.is_in_df(data_df, row):\n",
    "            num_in_df += 1\n",
    "    print('GPT-4', num_in_df / samples4_df.shape[0])\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = datasets[6]\n",
    "samples35_df = gpt35_samples[d ]\n",
    "samples4_df = gpt4_samples[d ]\n",
    "data_df = original_data[d ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples35_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples4_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best n-gram match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    samples35_df = gpt35_samples[dataset]\n",
    "    samples4_df = gpt4_samples[dataset]\n",
    "    data_df = original_data[dataset]\n",
    "        \n",
    "    print(dataset)\n",
    "    n_gram_distance = []\n",
    "    for i in range(samples35_df.shape[0]):\n",
    "        row = samples35_df.iloc[i]\n",
    "        min_dist, _ = analysis.find_matches(data_df, row, utils.strings_unequal)\n",
    "        n_gram_distance.append(min_dist)\n",
    "    print('GPT-3.5-turbo', len(data_df.columns)-np.mean(n_gram_distance), len(data_df.columns)) \n",
    "    \n",
    "\n",
    "    print(dataset)\n",
    "    n_gram_distance = []\n",
    "    for i in range(samples4_df.shape[0]):\n",
    "        row = samples4_df.iloc[i]\n",
    "        min_dist, _ = analysis.find_matches(data_df, row, utils.strings_unequal)\n",
    "        n_gram_distance.append(min_dist)\n",
    "    print('GPT-4', len(data_df.columns)-np.mean(n_gram_distance), len(data_df.columns)) \n",
    "\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### individual feature values from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    samples35_df = gpt35_samples[dataset]\n",
    "    samples4_df = gpt4_samples[dataset]\n",
    "    data_df = original_data[dataset]\n",
    "    feature_names = list(data_df.columns)\n",
    "        \n",
    "    print(dataset)\n",
    "    fvd = []\n",
    "    for i in range(100):\n",
    "        row = samples35_df.iloc[i]\n",
    "        for feature_name in feature_names:\n",
    "            min_dist, _ = analysis.find_matches(data_df, row[[feature_name]], utils.strings_unequal)\n",
    "            fvd.append(min_dist)\n",
    "    print('GPT-3.5-turbo', 100*(len(data_df.columns)-np.mean(fvd))/len(data_df.columns)) \n",
    "    \n",
    "\n",
    "    fvd = []\n",
    "    for i in range(100):\n",
    "        row = samples4_df.iloc[i]\n",
    "        for feature_name in feature_names:\n",
    "            min_dist, _ = analysis.find_matches(data_df, row[[feature_name]], utils.strings_unequal)\n",
    "            fvd.append(min_dist)\n",
    "    print('GPT-4', 100*(len(data_df.columns)-np.mean(fvd))/len(data_df.columns)) \n",
    "    print('-'*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
